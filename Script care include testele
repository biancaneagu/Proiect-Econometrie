############################################################################
# PROIECT ECONOMETRIE - APLICAȚIA 1 (VARIANTA COGNITIVE SCORE)
# TEMA: Determinanții Performanței Cognitive
############################################################################

rm(list = ls())

# --- 1. SETUP ---
# setwd("C:/Calea/Ta/Catre/Fisier") # <--- MODIFICĂ AICI DACA E NEVOIE

PackageNames <- c("tidyverse", "stargazer", "magrittr", "lmtest", "sandwich", 
                  "olsrr", "moments", "ggplot2", "tseries", "car", "caret", 
                  "MLmetrics", "factoextra", "cluster", "corrplot")

for(i in PackageNames){
  if(!require(i, character.only = T)){
    install.packages(i, dependencies = T)
    require(i, character.only = T)
  }
}

# --- 2. IMPORT ȘI PREGĂTIRE ---
raw_data <- read.csv("human_cognitive_performance.csv", header = TRUE)
df <- raw_data

# Transformări Dummy
df %<>% mutate(DummyGenderF = ifelse(Gender == "Female", 1, 0))
df %<>% mutate(DummyVegan = ifelse(Diet_Type == "Vegan", 1, 0))
df %<>% mutate(DummyExerciseHigh = ifelse(Exercise_Frequency == "High", 1, 0))

# !!! SCHIMBARE MAJORĂ AICI !!!
# Y = Cognitive_Score
# X = Sleep, Screen, Caffeine, Age, Dummies
df_final <- df %>% 
  dplyr::select(Cognitive_Score,     # <--- Noul Y
                Sleep_Duration,      
                Daily_Screen_Time,   
                Caffeine_Intake,     
                Age,                 
                DummyGenderF, DummyVegan, DummyExerciseHigh)

df_final <- na.omit(df_final)

# --- 3. ANALIZA EXPLORATORIE ---
stargazer(df_final, type = "text", title = "Statistici Descriptive (Cognitive Score)")

# Histograma Y
ggplot(df_final, aes(x = Cognitive_Score)) +
  geom_histogram(fill = "forestgreen", color = "white", bins = 15) +
  theme_minimal() + ggtitle("Distribuția Scorului Cognitiv")

# Matrice de Corelație
cor_matrix <- cor(df_final)
corrplot(cor_matrix, method = "number", type = "upper", tl.cex = 0.7)

# --- 4. CLUSTERING (K-MEANS OPTIMIZAT & STABIL) ---
# Vedem dacă există grupuri de performanță (ex: "High Performers" vs "Low Performers")

# Scalăm datele (obligatoriu pentru K-Means)
df_scaled <- scale(df_final)

set.seed(123)

# Calculăm clusterii folosind algoritmul "MacQueen" pentru a evita eroarea Quick-TRANSfer
km_res <- kmeans(df_scaled, 
                 centers = 3, 
                 nstart = 25, 
                 iter.max = 100,        # Creștem numărul de iterații
                 algorithm = "MacQueen") # <--- ASTA REZOLVĂ PROBLEMA

# Vizualizare pe eșantion mic (să nu crape memoria la grafic)
# Luăm maxim 2000 de puncte pentru desen
sample_n <- min(2000, nrow(df_scaled))
sample_idx <- sample(1:nrow(df_scaled), sample_n)

# Pregătim obiectul pentru vizualizare
km_viz <- list(data = df_scaled[sample_idx, ], cluster = km_res$cluster[sample_idx])
class(km_viz) <- "kmeans"

# Generăm graficul
fviz_cluster(km_viz, data = df_scaled[sample_idx, ],
             palette = "jco", 
             ggtheme = theme_minimal(), 
             geom = "point",
             main = "Tipologii de Performanță Cognitivă")

# --- 5. TRAIN / TEST SPLIT ---
set.seed(123)
# Atenție: împărțim pe baza Cognitive_Score acum
train_idx <- createDataPartition(df_final$Cognitive_Score, p = 0.8, list = FALSE)
train_data <- df_final[train_idx, ]
test_data  <- df_final[-train_idx, ]

# --- 6. MODELARE OLS (Y = Cognitive_Score) ---
model_ols <- lm(Cognitive_Score ~ Sleep_Duration + Daily_Screen_Time + Caffeine_Intake + 
                  Age + DummyGenderF + DummyVegan + DummyExerciseHigh, 
                data = train_data)

# Rezultate
stargazer(model_ols, type = "text", title = "Regresie: Determinanții Performanței Cognitive")

# =========================================================================
# CONTINUARE: VALIDARE ȘI PREDICȚIE (Cerințele 3.b și 3.c)
# =========================================================================

# 1. TESTE DE DIAGNOSTIC (Validarea ipotezelor)
cat("\n--- 1. Testul VIF (Multicoliniaritate) ---\n")
# Verificăm dacă variabilele sunt prea corelate între ele
vif_vals <- vif(model_ols)
print(vif_vals)
if(max(vif_vals) < 5) cat("Verdict: OK (Nu avem multicoliniaritate severă)\n")

cat("\n--- 2. Testul Breusch-Pagan (Homoscedasticitate) ---\n")
# H0: Erorile sunt constante (Homoscedasticitate)
# H1: Erorile sunt variabile (Heteroscedasticitate)
bp_test <- bptest(model_ols)
print(bp_test)

# --- CORECȚIE PENTRU HETEROSCEDASTICITATE ---
# Folosim estimatori robuști (Sandwich Estimator - HC1)
# Asta va ajusta "steluțele" și erorile standard, păstrând coeficienții la fel.

library(lmtest)
library(sandwich)

# Recalculăm tabelul de coeficienți cu erori robuste
model_robust <- coeftest(model_ols, vcov = vcovHC(model_ols, type = "HC1"))

# Afișăm noile rezultate corectate
print(model_robust)




cat("\n--- 3. Testul Jarque-Bera (Normalitate Reziduuri) ---\n")
# H0: Reziduurile sunt distribuite normal
resid_vals <- residuals(model_ols)
jb_test <- jarque.bera.test(resid_vals)
print(jb_test)
# NOTA: La 64.000 de observații, testele de normalitate pică des (p < 0.05).
# Te bazezi pe "Teorema Limitei Centrale" (CLT) care spune că OLS e valid pe eșantioane mari.

cat("\n--- 4. Testul Durbin-Watson (Autocorelare) ---\n")
# Valori intre 1.8 si 2.2 sunt ideale.
dw_test <- dwtest(model_ols)
print(dw_test)
